[MODEL_CONFIG]
epsilon = 1e-5
alpha = 0.99
reward_gamma = 0.99
MAX_GRAD_NORM = 5

MEMORY_CAPACITY = 1000000
BATCH_SIZE = 128
EPSILON_START = 0.99
EPSILON_END = 0.05
EPSILON_DECAY = 50000
target_update_freq = 4

actor_hidden_size = 128
critic_hidden_size = 128
critic_loss = mse

actor_lr = 5e-4
critic_lr = 5e-4
optimizer_type = rmsprop

ENTROPY_REG = 0.01

torch_seed = 0
shared_network = True
action_masking = True
state_split = True
reward_type = regionalR

[TRAIN_CONFIG]
MAX_EPISODES = 20000
EPISODES_BEFORE_TRAIN = 10
EVAL_EPISODES = 3
EVAL_INTERVAL = 200
reward_scale = 20.0
test_seeds = 0,25,50,75,100,125,150,175,200,325,350,375,400,425,450,475,500,525,550,575

[ENV_CONFIG]
seed = 0
simulation_frequency = 15
duration = 20
policy_frequency = 5

COLLISION_REWARD = -200
HIGH_SPEED_REWARD = 1
HEADWAY_COST = 4
HEADWAY_TIME = 1.2
MERGING_LANE_COST = 4

traffic_density = 1
